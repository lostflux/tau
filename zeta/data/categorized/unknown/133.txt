old id = 1119
Yann LeCun's Home Page
unknown
http://yann.lecun.com

Welcome to Yann's home page.
Yann LeCun,VP and Chief AI Scientist, FacebookSilver Professor of Computer Science, Data Science, Neural Science, and Electrical and Computer Engineering,New York University.
ACM Turing Award Laureate, (sounds like I'm bragging, but a condition of accepting the award is to write this next to you name)Member, National Academy of EngineeringNYU Affiliations:CILVR LabComputer Science Department, part of theCourant Institute of Mathematical Sciences,Center for Data ScienceCenter for Neural ScienceDepartment of Electrical and Computer Engineering,Facebook Affiliations:Facebook AI ResearchFacebook AINYU coordinates:Address: Room 516,60 Fifth Avenue, New York,NY 10011, USA.
Email: yann [ a t ] cs.nyu.edu (I may not respond right away)Phone: +1-212-998-3283 (I am very unlikely to respond or listen to voice mail in a timely manner)Administrative aide: Hong Tam +1-212-998-3374 hongtam [ a t ] cs.nyu.eduFacebook Coordinates:Address:770 Broadway, New York, NY 10003Email: yann [ a t ] fb.com (I may not respond right away)Executive assistant: Rocio Araujo: rocio [ a t ] fb.comFOR INVITATIONS TO SPEAK: please send email tolecuninvites[at]gmail.com(I really can't handle invitations sent to other email addresses)IF YOU REALLY NEED ME TO DO SOMETHING FOR YOU: (e.g. a review, a letter...) please send email to Rocio Araujorocio[at]fb.comPublications:Google Scholarslightly out of date list of publications with PDFs and DjVuNews/Blogs on Social Networks(I'm quite active there):Talks / Slide Decks:Slides of (most of my) talks:Videos:Playlists on YouTube:Biographies:bios of various lengths in English and FrenchMain Research Interests:AI, Machine Learning, Computer Vision, Robotics, and Computational Neuroscience. I am also interested Physics of Computation, and many applications of machine learning.
[stuff below this line is badly out of date]Seeresearch projects descriptions, lab member pages, events, demos, datasets...
We are working on a class of learning systems calledEnergy-Based Models, andDeep Belief Networks. We are also working on convolutional nets for visual recognition , and a type of graphical models known as factor graphs.
We have projects in computer vision, object detection, object recognition, mobile robotics, bio-informatics, biological image analysis, medical signal processing, signal processing, and financial prediction,....
Jump tomy course page at NYU, and see course descriptions, slides, course material...
See, watch and heartalks and tutorial.
Proposal for a new publishing model in Computer ScienceMany computer Science researchers are complaining that our emphasis on highly selective conference publications, and our double-blind reviewing system stifles innovation and slow the rate of progress of Science and technology.
This pamphlet proposes a new publishing model based on an open repository and open (but anonymous) reviews which creates a "market" between papers and reviewing entities.
MORE INFORMATION >>>>>Animals and humans can learn to see, perceive, act, and communicate with an efficiency that no Machine Learning method can approach. The brains of humans and animals are "deep", in the sense that each action is the result of a long chain of synaptic communications (many layers of processing). We are currently researching efficient learning algorithms for such "deep architectures". We are currently concentrating on unsupervised learning algorithms that can be used to produce deep hierarchies of features for visual recognition. We surmise that understanding deep learning will not only enable us to build more intelligent machines, but will also help us understand human intelligence and the mechanisms of human learning.
MORE INFORMATION >>>>>.
We are developing a new type of relational graphical models that can be applied to "structured regression problem". A prime example of structured regression problem is the prediction of house prices. The price of a house depends not only on the characteristics of the house, but also of the prices of similar houses in the neighborhood, or perhaps on hidden features of the neighborhood that influence them. Our relational regression model infers a hidden "desirability sruface" from which house prices are predicted.
MORE INFORMATION >>>>>.
My Lab, collaboration withNet-Scale Technologiesis one of 8 participants in the program (Applied Perception Inc., Georgia Tech, JPL, NIST, NYU/Net-Scale, SRI, U. Penn, Stanford).
Each LAGR team received identical copies of theLAGR robot, built be theCMU/NREC.
The robot is given the GPS coordinates of a goal to which it must drive as fast as possible. The terrain is unknown in advance. The robot is run three times through the test course.
The software can use the knowledge acquired during the early runs to improve the performance on the latter runs.
CLICK HERE FOR MORE INFORMATION, VIDEOS, PICTURES >>>>>.
Prior to the LAGR project, we worked on theDAVE project, an attempt to train a small mobile robot to drive autonomously in off-road environments by looking over the shoulder of a human operator.
CLICK HERE FOR INFORMATION ON THE DAVE PROJECT >>>>>.
Probabilistic models must be properly normalized, which sometimes requires evaluating intractable integrals over the space of all possible variable configurations. Since EBMs have no requirement for proper normalization, this problem is naturally circumvented. EBMs can be viewed as a form of non-probabilistic factor graphs, and they provide considerably more flexibility in the design of architectures and training criteria than probabilistic approaches.
CLICK HERE FOR MORE INFORMATION, PICTURES, PAPERS >>>>>.
I am developing learning systems that can recognize generic object purely from their shape, independently of pose and lighting.
SeeThe NORB dataset for generic object recognition isavailable for download.
CLICK HERE FOR MORE INFORMATION, PICTURES, PAPERS >>>>>.
Lush combines three languages in one: a very simple to use, loosely-typed interpreted language, a strongly-typed compiled language with the same syntax, and the C language, which can be freely mixed with the other languages within a single source file, and even within a single function.
Lush has a library of over 14,000 functions and classes, some of which are simple interfaces to popular libraries: vector/matrix/tensor algebra, linear algebra (LAPACK, BLAS), numerical function (GSL), 2D and 3D graphics (X, SDL, OpenGL, OpenRM, PostScipt), image processing, computer vision (OpenCV), machine learning (gblearning, Torch), regular expressions, audio processing (ALSA), and video grabbing (Video4linux).
If you do research and development insignal processing, image processing, machine learning, computer vision, bio-informatics, data mining, statistics, or artificial intelligence, and feel limited by Matlab and other existing tools,Lushis for you. If you want a simple environment to experiment withgraphics, video, and sound, Lush is for you. Lush is Free Software (GPL) and runs under GNU/Linux, Solaris, and Irix.
VISIT THE LUSH HOME PAGE >>>>My main research topic until I left AT&T was theDjVuproject. DjVu is a document format, a set of compression methods and a software platform for distributing scanned and digitally produced documents on the Web. DjVu image files of scanned documents are typically 3-8 times smaller than PDF or TIFF-groupIV for bitonal and 5-10 times smaller than PDF or JPEG for color (at 300 DPI). DjVu versions of digitally produced documents are more compact and render much faster than the PDF or PostScript versions.
Hundreds of websites around the worldare using DjVu for Web-based and CDROM-based document repositories and digital libraries.
My main research interest is machine learning, particularly how it applies to perception, and more particularly to visual perception.
I am currently working on two architectures for gradient-based perceptual learning:graph transformer networksandconvolutional networks.
Convolutional Nets are a special kind of neural net architecture designed to recognize images directly from pixel data. Convolutional Nets can be trained to detect, segment and recognize objects with excellent robustness to noise, and variations of position, scale, angle, and shape.
Have a look at the animateddemonstrations of LeNet-5, a Convolutional Nets trained to recognize handwritten digit strings.
Convolutional nets and graph transformer networks are embedded in several high speed scanners used by banks to read checks. A system I helped develop reads an estimated10 percent of all the checks written in the US.
Check outthis page, and/or readthis paperto learn more about Convolutional Nets and graph transformer networks.
TheMNIST databasecontains 60,000 training samples and 10,000 test samples of size-normalized handwritten digits. This database was derived from the original NIST databases.
MNIST is widely used by researchers as a benchmark for testing pattern recognition methods, and by students for class projects in pattern recognition, machine learning, and statistics.
I have several interests beside my family (my wife and three sons) and my research:My former group at AT&T (the Image Processing Research Department) and its ancestor (Larry Jackel's Adaptive Systems Research Department) made numerous contributions to Machine Learning, Image Compression, Pattern Recognition, Synthetic Persons (talking heads), and Neural-Net Hardware. Specific contributions not mentioned elsewhere on this site include the ever so popular Support Vector Machine, the PlayMail and Virt2Elle synthetic talking heads, the Net32K and ANNA neural net chips, and many others. Visitmy former group's home pagefor more details.
Links to interesting places on the web, friends' home pages, etc.
[HOME][NEWS][PUBLICATIONS][RESEARCH][DOWNLOADS][LENET][MUSIC][PHOTOS][HOBBIES][FUN][LINKS]Yann LeCun, ProfessorThe Courant Institute of Mathematical SciencesCopyright ï¿½ 2000-2018 Yann LeCun.
Yann LeCun, Le Cun, deep learning, ConvNet, CNN, LeNet, DjVu, convolutional neural networks, machine learning, computer vision, pattern recognition, document imaging, image compression, digital libraries,
